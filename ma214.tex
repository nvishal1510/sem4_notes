\documentclass{article}
\title{MA 214 - Introduction to Numerical Analysis}
\author{Vishal Neeli}
\date{}

\usepackage[a4paper, total={6in, 11in}]{geometry}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{grffile}
\usepackage{physics}
\usepackage{amsfonts}
\usepackage{amsthm}
\hypersetup{
	colorlinks=true,
	urlcolor=blue,
	linkcolor=cyan,
	filecolor=red
}
\newtheorem*{theorem}{Theorem}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% FOR CODE
% \usepackage{listings}
% \usepackage{color}

% \definecolor{dkgreen}{rgb}{0,0.6,0}
% \definecolor{gray}{rgb}{0.5,0.5,0.5}
% \definecolor{mauve}{rgb}{0.58,0,0.82}

% \lstset{frame=tb,
%   language=Java,
%   aboveskip=3mm,
%   belowskip=3mm,
%   showstringspaces=false,
%   columns=flexible,
%   basicstyle={\small\ttfamily},
%   numbers=none,
%   numberstyle=\tiny\color{gray},
%   keywordstyle=\color{blue},
%   commentstyle=\color{dkgreen},
%   stringstyle=\color{mauve},
%   breaklines=true,
%   breakatwhitespace=true,
%   tabsize=3
% }

\begin{document}
\maketitle

\section{Interpolation Theory}

\subsection{Introduction}

\begin{itemize}
	\item Given finite set of points, reconstructing the original curve is interpolation.
	\item There will be obviously infinitely many curve.

	\item Interpolation problem\\
	Given n+1 real distinct points: $x_0,x_1,\hdots,x_n$ and real numbers: $y_0,y_1,\hdots,y_n$

	Find a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that 
	\[f(x_i)= y_i \qquad \text{for }i=0,1,\hdots,n\]
	Such a function is called \textbf{interpolant} and points $x_i$ are called \textbf{interpolation points}.\\
	We attempt to rebuild original function using polynomial functions. This is called polynomial interpolation and function is polynomial interpolant.

	\item A polynomial is function of the form 
	\[p(x)=a_0 + a_1 x +\hdots + a_n x^n\]
	\item $\mathbb{P}_n$ is the set of polynomials consisting of all polynomials of degree $\leq n$\\
\end{itemize}

\subsection{Polynomial Interpolation}
	\begin{theorem}[Joseph-Louis Lagrange Theorem]
		Given $n+1$ data points with unique $x_i$s, then there exists a unique polynomial $p_n \in \mathbb{P}_n$ such that 
		\[p(x_i)=y_i \qquad \text{for } i= 0,1,\hdots,n \]\\
	\end{theorem}
	\begin{proof}
		 (1)  This can be shown by linear algebra. In a $n$ degree polynomial, we substitute the points and get $n+1$ equations in n+1 variables (coeff) and all the rows are unique (since $x_0,x_1,\hdots,x_n$ are unique), hence in $AX=b$, $|A|\neq0$. \\
	\end{proof}

	\begin{proof}
		(2)  Part 1: Uniqueness : If there is an interpolant, then the interpolant is unique\\
		Let there be 2 interpolants, $p_n$ and $q_n$ and let $r(x)=p(x)-q(n)$,\\
		\[r(x)=0 \text{ for } i=0,1,\hdots,n\] 
		This contradicts the fundamental theorem of Algebra. (A polynomial of degree n can have at most n real roots). Therefore 
		\[r(x)=0 \quad \forall x \in \mathbb{R}\]
		\[p(x)=q(x) \quad \forall x \in \mathbb{R}\]

		\noindent Part 2: Existence (construction):\\
			Given n+1 data points, build n+1 Langrange polynomials 
			\[L^n_k (x) =
			\begin{cases}
			 0 \qquad \text{for } i\neq k \\
			 1 \qquad \text{for } i=k
			\end{cases}
			 \]

		\[L^n_k(x)= \frac{(x-x_0)...(x-x_{k-1})(x-x_{k+1})...(x-x_n)}{(x_k-x_0)...(x_k-x_{k-1})(x_k-x_{k+1})...(x_k-x_n)}\]

		\[p(x) = \sum_{k=0}^n y_k L^n_k(x)\]\\
	\end{proof}

\subsection{Closeness between functions}
	Given two continuous functions $f,g:[a,b]\rightarrow \mathbb{R}$, to evaluate how close the functions are consider the following
	\[max_{x\in [a,b]} |f(x)-g(x)|\]


\subsection{Set of continuous Functions}
	$C[a,b]$ is the set of all continuous functions on [a,b]\\
	$C[a,b]$ is a infinite dimensional vector space
	\[f,g \in C \Longrightarrow f+g \in C \text{  and  }\lambda f\in C\]
	We define norm on $C[a,b]$ as
	\[||f|| = max_{x\in [a,b]} |f(x)|\]
	$C^{k}[a,b]$ denotes the set of all functions which are continuously k-times differentiable  



\subsection{Polynomial Approximation and Error}
	\begin{theorem}[Weierstrass approximation Theorem]
	\label{weierstrass}
		Given a function $f\in C[a,b]$ and given $\epsilon>0$, there exists a polynomial $p(x)$ such that,
		\[||f(x)-p||<\epsilon\]
	\end{theorem}

	\noindent\textbf{Using Langrange's recipe to approximate}\\
	Take $n+1$ interpolation points in the [a,b] and collect the function values at all the points. We have $n+1$ data points. Using Lagrange polynomials, find the interpolant\\
		
	\begin{theorem}[Error equation]
	\label{error}
		Let $f\in C^k[a,b]$, $x_0,x_1,\hdots,x_n \in [a,b]$ and $p\in \mathbb{P}_n$ be the interpolant using these points, then for all $x$, there exists a $\zeta = \zeta(x) \in (a,b)$ such that
		\[\boxed{f(x)-p(x) = \frac{1}{(n+1)!}f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)}\]
		\textbf{Note}: Here $\zeta$ is dependent on the x, i.e, for every x you choose, $\zeta$ generally changes.\\
	\end{theorem}

	\begin{proof}
		Consider the function,
		\[\psi(t)=(f(t)- p(t))\prod_{k=0}^n(x-x_k) - (f(x)-p(x))\prod_{k=0}^n(t-x_k)\]
		This $n+2$ roots (n+1 data points and x), applying rolle theorem's gives us that $\phi^{(1)}(t)$ has at least n+1 roots. Applying like this repeatedly on its derivatives, we get that $f^{(n+1)}$ has at least 1 root in $[a,b]$. Assuming the root to be $\zeta$. We have,
		\[f(x)-p(x) = \frac{1}{(n+1)!}f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)\]
	\end{proof}


	\noindent\textbf{Approximating the error}:\\
	Taking norm on both the sides of error equation, we have,
	\begin{align}
		\max_{x\in [a,b]} |f(x)-p(x)| &= \frac{1}{(n+1)!}||f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)||\\
		\max_{x\in [a,b]} |f(x)-p(x)| &\leq \frac{1}{(n+1)!} ||f^{(n+1)}||\text{ }max_{x \in [a,b]}\prod_{k=0}^n(x-x_k)
	\end{align}

	\noindent\textbf{Chebyshef interpolation points}:
		\[x_k= \frac{a+b}{2}+\frac{b-a}{2}cos\left(\frac{j\pi}{n}\right)\]
	\noindent These points minimise $max_{x \in [a,b]}\prod_{k=0}^n(x-x_k)$ and therefore prefered over equally spaced points on real line. These points can be visualised as projections of equally spaced points on the arc of the semicircle with $\frac{a+b}{2}$ as center and $\frac{(b-a)}{2}$ as radius.


\subsection{Some more methods for calculating interpolant}
	\begin{itemize}
		\item This is similar to the linear algebra method (given as proof(1) to Joseph-Louis Lagrange Theorem) for finding the interpolant.\\
		Consider the polynomial 
		\[p(x)= a_0 +a_1 (x-x_0) + a_2(x-x_0)(x-x_1)+ \hdots +a_n(x-x_0)...(x-x_{n-1})\]
		Find the coefficients $a_0, a_1, \hdots, a_n$ by substituting the data points. On substituting $x_0$, we get $a_0$, again on substituting $x_1$ and using $a_0$, we get $a_1$ and so on.

		\item Interpolant $p(x)$ of  $x_0,x_1,\hdots,x_n$ can be calculated using interpolants $a(x)$ and $b(x)$ of $x_1,x_2,\hdots,x_n$ and $x_0,x_1,\hdots,x_{n-1}$ respectively as 
			\[p(x)=\frac{(x-x_0)a(x)-(x-x_n)b(x)}{x_n-x_0}\]
	\end{itemize}

\subsection{Divided difference - recursion relation}
	\begin{itemize}
		\item \textbf{Divided difference} : It is the coefficient of $x_n$ in the interpolant $p \in \mathbb{P}_n$ and denoted by $f[x_0,x_1\hdots,x_n]$.\\
		Using Langrange polynomials, we have
		\[p(x)=\sum_{k=0}^n f(x_k) \prod_{j=0, j\neq k}^n \frac{x-x_j}{x_k-x_j}\]
		So the divided difference is
		\[f[x_0,x_1,\hdots,x_n]=\sum_{k=0}^n f(x_k) \prod_{j=0, j\neq k}^n \frac{1}{x_k-x_j}\]

	\end{itemize}

	\begin{theorem}[Divided difference recursion theorem]
		\[\boxed{f[x_0,x_1,\hdots,x_{m+1}] = \frac{[f[x_1, x_2,\hdots,x_{m+1}] - f[x_0,x_1,\hdots,x_{m}]}{x_{m+1}-x_0}}\]
	\end{theorem}
	\begin{proof}
		Let p(x) be the interpolant for $x_0,x_1,\hdots,x_m$ and q(x) be the interpolant for $x_1,x_2\hdots,x_{m+1}$. Then,
		\[L(x)= \frac{(x-x_0)q(x)+(x_{m+1}-x)p(x)}{x_{m+1}-x_0}\] is an interpolant.
		Since, interpolant is unique, considering coeff of $x_{m}$ we have,
		\[f[x_0,x_1,\hdots, x_m]=  \frac{f[x_1, x_2,\hdots,x_{m+1}] - f[x_0,x_1,\hdots,x_{m}]}{x_{m+1}-x_0}\]
	\end{proof}

	\begin{theorem}[Interpolant using divided differences]
		Suppose $x_0,x_1,\hdots,x_n$ be the data points. Then interpolant $p \in \mathbb{P}_n$ is
		\[\boxed{p(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_n] \prod_{j=0}^{n-1}(x-x_j)}\]
	\end{theorem}

	\begin{proof}
		We prove this by induction. Base case $n=0$ is trivially satisfied.
		Assume that this is satisfied for $p_{k}$,
		\[p_k(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_k] \prod_{j=0}^{k-1}(x-x_j)\]
		Consider the polynomial $p_{k+1}(x)-p_k(x) \in \mathbb{P}_{k+1}$ which has $x_0,x_1,\hdots,x_k$ as roots. Hence,
		\[p_{k+1}(x)-p_k(x) = c\prod_{j=0}^k(x-x_j)\]
		Comparing leading coefficient on both sides, we have $c=f[x_0,x_1,\hdots,x_k]$. Hence,
		\[p_{k+1}(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_{k+1}] \prod_{j=0}^{k}(x-x_j)\]
		By PMI,
		\[p(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_n] \prod_{j=0}^{n-1}(x-x_j)\]
	\end{proof}


% \subsection{Time complexity of the algorithms}
% 	% (Slide is useless)
% 	\begin{itemize}
% 		\item Langrange's method - $O(n^2)$ : Computing each Langrange polynomial can be done in $O(n)$ (Finding the coefficients given roots can be done in \href{https://cs.stackexchange.com/questions/116643/what-is-the-most-efficient-algorithm-to-compute-polynomial-coefficients-from-its}{$O(n log^2(n))$}). We need to do to this n times. So, $O(n^2)$.
% 		\item Divided differences - $O(n^2)$ : Summing operations in each stage - $n+(n-1)+ (n-2)+\hdots+ 1$. Hence, $O(n^2)$.
% 		\item Divided difference can be considered better because we can extend from n to n+1 and so on without discarding previous computation.
% 	\end{itemize}


\subsection{Weierstrass theorem consequences}
\begin{itemize}
	\item In \nameref{weierstrass}, take $\epsilon_n = \frac{1}{n}$. Then weierstrass theorem proves the existence of sequence of polynomials $p^{(1)}, p^{(2)}, \hdots$ such that 
		\[\lim_{n\rightarrow \infty} ||f-p^{(n)}||=0\]
	\item If \textbf{f in not a polynomial}, then 
		\[\lim_{n\rightarrow \infty} \text{degree of }p{(n)} = 0\]
		% To prove this (crudely), assume that there exist a non-polynomial function f such that $||f-p^{(n)}||=0$, then $f=p^{(n)}$ which contradicts the assumtion that f is not a polynomial.
\end{itemize}


\subsection{Spline Interpolation}
\label{spline}
\begin{itemize}
	\item \textbf{Piece wise polynomial}: $\phi \in C[a,b]$ is a piecewise polynomial function, if there exists $a=x_0<x_1<\hdots<x_n=b$ such that $\phi \in \mathbb{P}_m$ when $x \in [x_i,x_{i+1}]$ for all $i=0,1,\hdots,n$ and some $m>0$.


	\item Piece wise polynomial $\phi$ need not be polynomial in whole domain.

	\item Splines interpolation for $f \in C[a,b]$
	\begin{itemize}
		\item Pick some data points $x_0,x_1,\hdots,x_n$ such that $a=x_0<x_1<\hdots<x_n=b$
		\item Fix $m\leq n$
		\item Build $\phi$ in each subinterval $[x_i,x_{i+1}]$ using the following conditions:
			\[\phi(x_i)= f_i \qquad \text{for } i=0,1,\hdots,n\]
			\[\lim_{h\rightarrow 0+} \phi(x_i-h) = \lim_{h\rightarrow 0+} \phi(x_i+h) \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\lim_{h\rightarrow 0+} \dv{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\lim_{h\rightarrow 0+} \dv[2]{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv[2]{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\vdots\]
			\[\lim_{h\rightarrow 0+} \dv[m-1]{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv[m-1]{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
		\item We have $(n+1)+m(n-1)=n(m+1)-(m-1)$ conditions. We need $m-1$ more conditions.
	\end{itemize}

\end{itemize}


\subsection{Linear Splines}

	\textbf{Constructing linear splines}:\\
	Since the degree is only 1, we can constructs the splines using the equation of straight lines between the knots (data points).

	\begin{align*}
		s_0 &= \frac{f_1 - f_0}{x_1 - x_0}x + \frac{x_1 f_0 - x_0f_1}{x_1-x_0}		&\text{for } x\in [x_0,x_1]\\	
		s_1 &= \frac{f_2 - f_1}{x_2 - x_1}x + \frac{x_2 f_1 - x_1f_2}{x_2-x_1}		&\text{for } x\in [x_1,x_2]\\	
		&\vdots\\
		s_{n-1} &= \frac{f_n - f_{n-1}}{x_n - x_{n-1}}x + \frac{x_n f_{n-1} - x_{n-1}f_n}{x_n-x_{n-1}}		&\text{for } x\in [x_{n-1},x_n]\\	
	\end{align*}

	\begin{theorem}[Linear Splines error]
	\label{linear}
		Let $f\in C^2[a,b]$ and $s_L(x)$ be the interpolating \textbf{linear spline} at (n+1) knots $a=x_0<x_1<\hdots<x_n=b$ and let $h$ be the maximum subinterval length, then
			\[\boxed{||f-s_L|| \leq \frac{h^2}{8} ||f''||}\] 
	\end{theorem}
	\begin{proof}
		Consider the interval $[x_i,x_{i+1}]$, then $s_L (x)$ is the interpolating polynomial in this interval. Using the error equation for interpolating polynomials,
		\[f(x)-s_L(x)=\frac{1}{2} f''(\zeta) (x-x_i)(x-x_{i+1})\]
		Taking absolute value on both the sides,
		\begin{align*}
			|f(x)-s_L(x)| &= \frac{1}{2} |f''(\zeta)| |(x-x_i)(x-x_{i+1})|\\
						&\leq \frac{1}{2} ||f''|| \frac{h_i^2}{4} \qquad \text{where $h_i = \frac{x_{i+1}-x_i}{2}$ }\\
						&\leq \frac{h_i^2}{8} ||f''||
		\end{align*}
		Considering $h=max (h_i)$, then for $x\in [a,b]$
			\[max_{x\in [a,b]}|f(x)-s_L(x)| \leq \frac{1}{8} h^2||f''||\]

	\end{proof}



\subsection{Cubic Splines}
	\textbf{Constructing cubic splines}:\\
	\[s_i(x) = a_ix^3 + b_ix^2 + c_ix +d_i\]
	Using the conditions given in the section \ref{spline}, we have 4n-2 equations to for 4n coefficients. We choose the other two conditions as 
	\[s_0''(x_0) = s_{n-1}''(x_n)=0\]
	We have 4n variables (coefficients) and 4n equations, i.e, we have a $4n\times4n$ matrix which can be solved to get the coefficients of the spline.\\


	We can simplify this by choosing the form of spline as
	\[s_i(x) = a_i(x-x_i)^3 + b_i(x-x_i)^2 + c_i(x-x_i) +d_i \qquad \text{for }i=0,1,\hdots,n-1\]
	We will work only with \textbf{equally spaced knots}, i.e, $x_{i+1}-x_{i} = const$ for $i = 0, 1,\hdots,n-1$.
	We also define 
	\[\sigma_i = s''(x_i) \qquad \text{for }i=0,1,\hdots,n\]
	After doing a lot of simplification, we get 
	\begin{gather*}
		\boxed{a_i = \frac{\sigma_{i+1}-\sigma_i}{6h}}\\
		\boxed{b_i = \frac{\sigma_i}{2}}\\
		\boxed{c_i = \frac{f_{i+1}-f_i}{h} - \frac{h}{6}(2\sigma_i+\sigma_{i+1})}\\
		\boxed{d_i = f_i}
	\end{gather*}
	$\sigma_i$ s can be obtained by solving these equations
	\[\boxed{\sigma_{i-1} + 4 \sigma_i + \sigma_{i+1} = \frac{6}{h^2}(f_{i-1} - 2f_i + f_{i+1} )}\]
	This can be put in matrix form as
	\begin{equation*}
	\begin{bmatrix}
	4 &1 &0 &\hdots &0 &0 &0\\
	1 &4 &1 &\hdots &0 &0 &0\\
	0 &1 &4 &\hdots &0 &0 &0\\
	\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots\\
	0 &0 &0 &\hdots &4 &1 &0\\
	0 &0 &0 &\hdots &1 &4 &1\\
	0 &0 &0 &\hdots &0 &1 &4
	\end{bmatrix} 
	\begin{bmatrix}
	\sigma_1\\
	\sigma_2\\
	\sigma_3\\
	\vdots\\
	\sigma_{n-3}\\
	\sigma_{n-2}\\
	\sigma_{n-1}
	\end{bmatrix}
	= \frac{6}{h^2}
	\begin{bmatrix}
	f_0 - 2f_1 +f_2\\
	f_1 -2f_2 +f_3\\
	f_2 -2f_3 +f_4\\
	\vdots\\
	f_{n-4} -2f_{n-3} +f_{n-2}\\
	f_{n-3} -2f_{n-2} +f_{n-1}\\
	f_{n-2} -2f_{n-1} +f_{n}
	\end{bmatrix}
	\end{equation*}


	\begin{theorem}[Error - Cubic Splines (equispaced knots)]
		Let $f\in C^4[a,b]$ and $s(x) \in C^2[a,b]$ be the interpolating \textbf{natural cubic spline} at (n+1) \textbf{equispaced knots} $a=x_0<x_1<\hdots<x_n=b$ and let $h$ be the subinterval length ($h=x_{i+1}-x_i$), then
			\[\boxed{||f-s|| \leq C||f^{(4)}|| h^4 \qquad \text{for some } C>0 }\] 
	\end{theorem}
	\begin{proof}
		Consider the function g which is defined as $g=f-s_i$ on each subinterval $[x_i,x_{i+1}]$.
		Then $g(x_i)=0$ for $i=0,1,\hdots,n-1$.\\
		We can see that the zero polynomial is the linear spline of $g(x)$ and using the theorem \nameref{linear}, we have
		\begin{align*}
			||g-0|| &\leq \frac{h^2}{8} ||g''||\\
            ||f-s|| &\leq \frac{h^2}{8} ||f'' - s''||
		\end{align*}
		Assuming that $||f''-s''||\leq C h^2 ||f^{(4)}||$, we have
		\[||f-s|| \leq C h^4 ||f^{(4)}|| \qquad \text{for some } C> 0\]
		
	\end{proof}


\section{Numerical Integration}
	\subsection{Introduction}
		\begin{itemize}
			\item Given f be a real valued function, we want to evaluate 
			\[\int_a^b f(x) dx\]

			\item If we can find its antiderivative $F(x)$, then we can use the fundamental theorem of calculus and evaluate it. But finding antiderivate is not so straightforward.
			\[\int_a^b f(x) dx = F(b) - F(a)\]
		\end{itemize}

	\subsection{Newton-Cotes Formula}
		Let $f(x):[a,b]\rightarrow \mathbb{R}$ and $p(x) \in \mathbb{P}_n$ be the polynomial interpolant using the data points $a=x_1<x_2<\hdots<x_n=b$, then the definite integral $\int_a^b f(x) dx$ can be approximated as 

		\begin{align*}
			\int_a^b f(x) dx &\approx \int_a^b p(x) dx\\
							 &= \int_a^b \sum_{i=0}^n f(x_i) L_i(x) dx\\
							 &= \sum_{i=0}^n f(x_i) \int_a^b L_i(x) dx
		\end{align*}
		Let $x_i = a+ih$ for $i=0,1,\hdots,n$ and $x= a + th$ for $t\in [0,n]$, then  we have,
		\[\int_a^b L_i(x) dx= \int_a^b \prod_{k=0, k\neq i}^n \frac{x-x_k}{x_i-x_k} dx = \int_0^n  \prod_{k=0, k\neq i}^n \frac{t-k}{i-k} h dt = h\int_0^n \varphi_i(t) dt = h w_i\]
		\[\text{where }w_i = \int_0^n \varphi_i(t) dt\text{    and   }\varphi_i(t) = \prod_{k=0, k\neq i}^n \frac{t-k}{i-k}\]
		% \noindent $w_i$ is also known as \textbf{quadrature weights}.
		\noindent Then,
		\[\boxed{\int_a^b f(x)dx \approx h \sum_{i=0}^n w_i f(x_i)} \]
		\textbf{Note}: $w_i$s are independent of f, end points a and b and h. $w_i$s are dependent on only dependent on n.
		\textbf{Note}: $w_i$s are symmetric i.e, 
		\[w_k = w_{n-k}\]

		\noindent \textbf{Trapezium rule} ($n=1$):
			\[\int_a^b f(x)dx \approx \frac{b-a}{2} (f(a)+f(b))\]
		\textbf{Simpson's rule} ($n=2$):
			\[\int_a^b f(x)dx \approx \frac{h}{3} \left(f(a) + 4f\left(\frac{a+b}{2}\right)+ f(b)\right)\]


	\subsection{Newton-Cones formula error}
	We use the \nameref{error}, for interpolation polynomial to calculate the error in Newton-Cotes formula.
		\begin{align*}
			|I_f - I_P|  &= \left|\int_a^b f(x)-p(x) dx \right|\\
						 &\leq \int_a^b |f(x)-p(x)|dx\\
			\end{align*}
		 \[\boxed{|I_f - I_P|  \leq \frac{1}{(n+1)!} ||f^{(n+1)}|| \int_a^b \prod_{i=0}^n |x-x_i| dx}\]

		For trapezium rule (n=1)
		\begin{align*}
		|I_f - I_{P_1}|  &\leq \frac{1}{2} ||f''|| \int_a^b |(x-a)(x-b)|dx\\
							&= \frac{1}{12} ||f''||{(b-a)^3}
		\end{align*}
		For simpson's rule (n=2)
		\begin{align*}
		|I_f - I_{P_2}|  &\leq \frac{1}{6} ||f'''|| \int_a^b |(x-a)(x-\frac{a+b}{2})(x-b)|dx\\
							&= \frac{1}{192} ||f''||{(b-a)^4}
		\end{align*}

		% (//todo: stricter bound for siemsons rule)

	\subsection{Convergence of the approximation}
	\begin{itemize}
	\item The difference $|I_f - I_{P_n}|$ \textbf{does not converge} to 0 as we increase n. This is (crudely) because the weights sometimes takes negative values.
	\item A similar approximation which converges as n increases is Gaussian quadratures.
		\[G_n(f)= \sum_{i=0}^n W_i f(x_i)\]
	where weights $W_i$ are
		\[W_i = \int_a^b (L_i(x))^2dx = \int_a^b \left[\prod_{k=0,k\neq i}^n \frac{x-x_k}{x_i - x_k} \right]^2 dx\]
	Here, the points $x_k$s are \textbf{not} equally spaces.\\
	$x_k$s are the roots of Legendre Polynomials
	\end{itemize}


	\subsection{Composites Rule} 
	Similar to the splines case, we approximate the integral using Newton-Cotes formula in each subinterval.
	\begin{itemize}
		\item \textbf{Composite Trapezoidal Rule}: We divide the interval $[a,b]$ into m subintervals and apply trapezoidal rule in each subinterval.
		\begin{align*}
			\int_a^b f(x)dx &\approx \int_a^b C_{p_1} dx\\
					&= h(\frac{1}{2} f(a) + \frac{1}{2} f(a+h)) + h(\frac{1}{2} f(a+h) + \frac{1}{2} f(a+2h))+\hdots + h(\frac{1}{2} f(a+(m-1)h) + \frac{1}{2} f(b))\\	
				&= h(\frac{1}{2}f(a) + f(a+h) +f(a+2h)+ \hdots + f(a+(m-1)h) + \frac{1}{2} f(b))
		\end{align*}
		\item \textbf{Composite Simpson's Rule}: We divide the interval $[a,b]$ into 2m subintervals and apply simpons' rule.
		\begin{align*}
			\int_a^b f(x)dx &\approx \int_a^b C_{p_2}dx\\
			&= h(\frac{1}{3} f(a) + \frac{4}{3} f(a+h) + \frac{1}{3} f(a+2h)) + h(\frac{1}{3} f(a+2h) + \frac{4}{3} f(a+3h) + \frac{1}{3}f(a+4h))\\
			&+\hdots + h(\frac{1}{3} f(a+(2m-2)h) + \frac{4}{3} f(a+(2m-1)h) + \frac{1}{3} f(b))\\
				&= h(\frac{1}{3}f(a) + \frac{4}{3}f(a+h) + \frac{2}{3}f(a+2h) + \frac{4}{3}f(a+3h) +\frac{2}{3}f(a+4h) + \hdots \\
				&+\frac{2}{3}f(a+(2m-2)h) +\frac{4}{3}f(a+(2m-1)h) + \frac{1}{3} f(b))
		\end{align*}

	\end{itemize}

	\subsection{Error - Composites Rule}
	\begin{itemize}
	\item To get an estimate of the error $|I_f - C_{p_n}|$, we sum the error in each subinterval. 
	\item For composite trapezoid rule, 
	\begin{align*}
		|I_f - C_{p_1}| &\leq \sum_{i=0}^m \frac{1}{12}||f''||h^3\\
						&= \frac{1}{12} ||f''|| h^2 (b-a)
	\end{align*}
	\end{itemize}


\section{Ordinary Differential Equation}
	\subsection{Uniqueness and Existence Theorem}
	\begin{theorem}[Cauchy-Lipschitz-Picard]
		Consider the initial value problem 
		\[y' = f(t,y) \qquad y(t_0) = y_0 \]
		\begin{enumerate}
			\item If the function f is continuous for $t_0\leq t \leq T$ and $y_0 -C \leq y \leq y_0 +C$ for some constants T, $C>0$.
			\item If the function f satisfies lipschitz condition. \\
			\textbf{Lipschitz condition }: There exists a constant $L>0$ such that 
			\[|f(t,u)-f(t,v)|\leq L|u-v| \]
			for all $t\in [t_0, T]$ and $u,v \in [y_0 - C, y_0 + C]$
		\end{enumerate}
		Then there exists a unique solution $y(t) \in C^1[t_0,T]$ such that 
		\[y' = f(t,y) \qquad y(t_0) = y_0\]
	\end{theorem}
	Lipschitz condition imposes the condition that the slope of the function f (with t treated as constant) is bounded. 


	\subsection{Integral Formula}
		Consider the initial value problem
		\[y' = f(t,y) \qquad y(t_0) = y_0 \]
		Interating with y from $y_0$ to y and t from $t_0$ to t
		\[y = y_0 + \int_{t_0}^t f(s,y(s))ds \]

	\subsection{Order of Numerical Method}
		Consider a numerical method given by the recurrence relation
		\[y_{i+1} = F(t,f, y_0, y_1,\hdots,y_i, y_{i+1})\]
		Order of the numerical method is said to be p if
		\[y(t_{i+1}) - F(t,f,y(t_0), y(t_1), \hdots, y(t_i),y(t_{i+1})) = O(h^{p+1})\] 

	This in some sense represents order of error that is included while approximating from $y_i$ to $y_{i+1}$ since we use exact values while computing $y_{i+1}$ and get the difference between $y(t_{i+1})$ and $t_{i+1}$. \\
	Order can also be intepreted as the highest degree of polynomial which is exactly recovered after approximating the solution using the numerical method. (Here, order is the degree of the polynomial and not function f)

	\subsection{Global error}
		In an interval [0,T] for a given $h>0$, there are $\left[\frac{T}{h} \right]+1$ are equally spaced mesh points. Let 
		\[e_{n,h} = y_{n,h} - y(t_n)\]

	\subsection{Convergence of Numerical Method}
		A numerical method is said to be convergent if 
		\[\lim_{h\rightarrow0^+} max_{i=0,1,\hdots,[\frac{T}{h}]} |e_{i,h}| = 0\]

	\subsection{Euler Method}
		To approximate the solution, we divide the interval [0,T] into n mesh points, i.e, $t_i = i h$, where $h= \frac{T}{n}$
		At the mesh point $t_i$, we find approximations $y_i$ for exact solution $y(t_i) = y(t_{i-1}) + \int_{t_{i-1}}^{t_{i}} f(s,y(s)) ds$ using
		\[y_i = y_{i-1} + h f(t_{i-1},y_{i-1}) \qquad \text{for i} = 1,2,\hdots,n \]

		\textbf{Order}:
		\begin{align*}
			&y(t_{i+1}) - F(t,f,y(t_0), y(t_1), \hdots, y(t_n),y(t_{n+1})) \\
			&= y(t_{i+1}) -( y(t_i) + h f(t_i, y(t_i)) )\\
			&= y(t_i) + h y'(t_i) + \frac{h^2}{2} y''(\zeta) - (y(t_i) + h f(t_i, y(t_i) )) \qquad \text{using taylor's expansion}\\
			&= \frac{y''(\zeta)}{2} h^2\\
			&= O(h^2)
		\end{align*}
		Hence, the order of Euler's method is 1.

		Using the above result and lipschitz condition, we can prove that 
		\[|e_{i,h}| \leq \frac{Ch}{\lambda}((1+\lambda h)^i -1) \qquad \text{for i } = 0,1,\hdots,[\frac{T}{h}] \]

		Since, $|e_{i,h}| = O(h)$, so $\lim_{h\rightarrow} |e_{i,h}| =0$ and hence euler's method is convergent. Since the global error converges to zero at the rate of $h^1$, the order of convergence of Euler method is said to be 1.

	\subsection{Trapezoidal Rule}
		The recurrence relation for trapezoidal rule is implicit.
		\[y_i = y_{i-1} + \frac{h}{2} (f(t_{i-1}, y_{i-1})+ f(t_i, y_i)) \]

		\textbf{Order}:
		\begin{align*}
			&y(t_{i+1}) - F(t,f,y(t_0), y(t_1), \hdots, y(t_n),y(t_{n+1})) \\
			&= y(t_{i+1}) - (y(t_i) + \frac{h}{2} (y'(t_i)+ y'(t_{i+1})))\\
			&= y(t_i)+hy'(t_i)+\frac{h^2}{2}y''(t_i) + \frac{h^3}{6} y'''(\zeta_1) - (y(t_i)+ \frac{h}{2}(y'(t_i) + y'(t_i) + h y''(t_i) + \frac{h^2}{2} y'''(\zeta_2)))\\
			&= (\frac{y'''(\zeta_1)}{6} - \frac{y'''(\zeta_2)}{4})h^3\\
			&= O(h^3)
		\end{align*}
		Hence, the order of Trapezoidal Rule is 2.

		Using the above result and lipschitz condition, we can prove that
		\[|e_{i,h}| \leq \frac{Ch^2}{\lambda} \left( \left( \frac{1 + \frac{\lambda h}{2}}{1 - \frac{\lambda h}{2}}\right)^i -1 \right) \qquad \text{for i } = 0,1,\hdots, [\frac{T}{h}]\]

		Since, $|e_{i,h}| = O(h^2)$, so $\lim_{h\rightarrow} |e_{i,h}| =0$ and hence trapezoidal method is convergent. Since the global error converges to zero at the rate of $h^2$, the order of convergence of trapezoidal method is said to be 2.



\pagebreak


	\subsection{Multistep Methods}
	\begin{itemize}
		\item \textbf{Simpson's Rule}
			We have 
			\begin{align*}
				y(t_{i+1})- y(t_{i-1}) &= \int_{t_{i-1}}^{t_{i+1}} f(t,y)dt
			\end{align*}

			We use the simpsons rule to approximate the integral,
			\begin{equation*}
				\boxed{y_{t_{i+1}} = y_{t_{i-1}} + \frac{h}{3}\left( f(t_{i-1},y_{i-1} + 4 f(t_i,y_i)+ f(t_{i+1},y_{i+1}) \right)}
			\end{equation*}

		\textbf{Note:} Here, $h = t_{i+2}- t_i$

		\item \textbf{Adam Bashfort Method:} (Explicit 4 step method)
		\[y_{i+4} = y_{i+3} + \frac{h}{24}(55f(t_{i+3},y_{i+3})- 59 f(t_{i+2},y_{i+2}+ 37 f(t_{i+1},y_{i+1}-9f(t_i,y_i)))) \]
		\item \textbf{Adam Moulton Method:} (Implicit 3 step method)
		\[y_{i+3} = y_{i+2} + \frac{h}{24}(9f(t_{i+3},y_{i+3})+ 19 f(t_{i+2},y_{i+2}-5 f(t_{i+1},y_{i+1}-9f(t_i,y_i)))) \]

	\end{itemize}

	\subsection{General multistep method}
		Any s step method is of the form 
		\[\sum_{m=0}^s a_m y_{i+m} = h\sum_{m=0}^s b_m f(t_{i+m},y_{i+m}) \qquad\text{for i}=0,1,2,\hdots\]

		Here, $a_m, b_m, s$ are constants (independent of h,i and f)\\
		For any s step method we need s starting points ($y_0,y_1,y_2,\hdots,y_{s-1}$)

	\subsection{Constructing s step method}
		Adams method of constructing s step method: 
		\begin{enumerate}
			\item Take the s points $t_i,t_{i+1},\hdots,t_{i+s-1}$ and corresponding values (approximated by using $y_i$ instead of $y(t_i)$) $f(t_i,y_i), f(t_{i+1},y_{i+1}), \hdots,f(t_{i+s-1},y_{i+s-1})$ and find the interpolating polynomial
			\item Integrate the obtained polynomial between $t_{i+s-1}$ to $t_{i+s}$ for obtaining $y_{i+s}- y_{i+s-1}$, i.e,
			\[y_{i+s}- y_{i+s-1} = \int_{t_{i+s-1}}^{t_{i+s}} \sum_{m=0}^{s-1} f(t_{i+m},y_{i+m}) \prod_{p=0,p\neq m}^{s-1} \left( \frac{t-t_{i+p}}{t_{i+m} - t_{i+p}}\right)dt \]
			On simplifying we have 
			\[y_{i+s}- y_{i+s-1} =  h\sum_{m=0}^{s-1} b_m f(t_{i+m},y_{i+m}) \]
			where $b_m = \int_{s-1}^{s}\prod_{p=0,p\neq m}^{s-1} \left( \frac{x-p}{m - p}\right)dx$
		\end{enumerate}

	\subsection{Order of s step method}
	\begin{theorem}
		The order of s step method is p if
		\begin{enumerate}
			\item \begin{flalign*}
				&\sum_{m=0}^s a_m =0&
			\end{flalign*}
			\item \begin{flalign*}
				&\sum_{m=0}^s a_m m^k = k \sum_{m=0}^s b_m m^{k-1} \qquad \text{for }k =1,2,\hdots,p&
			\end{flalign*}
			\item \begin{flalign*}
				&\sum_{m=0}^s a_m m^{p+1} \neq (p+1) \sum_{m=0}^s b_m m^{p} &
			\end{flalign*}
		\end{enumerate}
	\end{theorem}
	So to compute order, we keep on checking if $\sum_{m=0}^s a_m m^k = k \sum_{m=0}^s b_m m^{k-1}$ while increasing, if this condition is not satisfied for k (first time), then $k-1$ is the required order.

	\subsection{Convergence}
	Consider the general s step method
	\[\sum_{m=0}^s a_m y_{i+m} = h\sum_{m=0}^s b_m f(t_{i+m},y_{i+m}) \qquad\text{for i}=0,1,2,\hdots\]
	The associated characteristics polynomials are
	\[\rho(z) = \sum_{m=0}^s a_m z^m \qquad\qquad \sigma(z) = \sum_{m=0}^s b_m z^m\]

	\begin{theorem}[Dahlquist equivalence theorem]
	A s-step method is convergent iff
	\begin{enumerate}
		\item The method is of order $p\geq0$
		\item The roots of the characteristic polynomial $\rho(z)$ lies in the closed unit disc in the complex plane, with any roots that lie on the unit circle being simple.
	\end{enumerate}
	
	\end{theorem}



\pagebreak

\section{Numerical Solutions to system of Linear Equations}
	
	\subsection{Gauss Elimination}
	({I skipped some details})
		\begin{enumerate}
			\item We transform the given matrix (augumented matrix) into row echelon form using elementary row transformations, i.e, we eliminate the variable $x_i$ from all the equations below it by multiplying with appropriate factor and subtracting. 
			\item Solve the system by back substitution, i.e, start from the last row variable and work your way up.
			\[x_i = \frac{b_i - \sum_{j=i+1}^n a_{ij}}{a_{ii}}\]
		\end{enumerate}

	\subsection*{Factorisation Methods}
		Instead of solving $Ax = B$, we factorise $A=BC$, then solve for $y\in R^n$ in $By = b$ and then $x\in R^n$ in $Cx = y$. The trick is that B and C should be "simple" matrices, so that we can simplify our problem.

	\subsection{LU Factorisation}

		\textbf{Lower and Upper triangular matrices}
		\begin{itemize}
			\item If a lower triangular matrix is invertible, then the inverse is also a lower triangular matrix with diagonal entries being reciprocals of the original diagonal entries. Same goes with upper triangular matrices
			\item The product of two lower triangualar matrices is also a lower triangular matrix. Same goes with upper triangular.
		\end{itemize}

		We factorise A into 
			\[A = LU\]
		$L$ is lower triangular matrix with diagonal entries equal to 1 and $U$ is upper-triangular matrix.

		\begin{itemize}
			\item This is same as Gauss elimination method.
			\item We obtain the $L$ matrix and $U$ matrix from the process. 
			\item L matrix is the matrix of multipliers, i.e, the numbers we multiply the above rows to get rid of the rows below. $m_{ij}$ is the number we multiply the above row to get rid of $j$th variable in the $i$th column.
		\end{itemize}

		\begin{theorem}
		Let A be a matrix such that all of its diagonal submatrices are invertible. Then there {exists} a unique pair of matrices L and U with L being lower-triangular with diagonal entries equal to unity and U being upper-triangular such that
		\[A= LU\]
		\end{theorem}

	\subsection{Chelosky Factorisation}
		\textbf{Positive Definite}:\\
		Let A be a real, symmetric matrix, then the following conditions are equivalent:
		\begin{enumerate}
		\item A is positive definite
		\item $\langle Ay, y \rangle \geq 0$  for all $y\in R^n$ 
		\item All the eigen values of A are positive.
		\item All the pivots are positive
		\item The determinates of all the diagonal submatrices are positive
		\end{enumerate}
		Ref : \href{http://slpl.cse.nsysu.edu.tw/chiaping/la/pdm.pdf}{Positive definite matrices}

		\begin{theorem}
			Let A be a \emph{real, symmetric and positive definite matrix}, then there exists a unique lower triangular matrix B with positive diagonal entries such that 
			\[A = BB^T\]
			B is called the Chelosky factor
		\end{theorem}

		\textbf{Practical approach}: Take matrix B in terms of variables $b_{ij}$ and multiply with $B^T$. Compare the terms to find the unknowns (prefably left to right).

	\subsection{QR Factorisation}
		In this we factor as 
		\[A = QR\]
		where Q is a orthogonal matrix ($Q^T = Q$) and R is a upper triangular matrix

		\begin{theorem}
			Every invertible matrix can be factorised as $A= QR$
		\end{theorem}
	(//todo: Practical approach)

\section{Norms}

\subsection{Norms on $R^n$}
	\begin{itemize}
	\item $l^p$ norm for $p \in [1,\infty]$
		\[||x||_{l^p} = (\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}} \]
	\item $l^\infty$ norm 
		\[||x||_{l^\infty} = \max_{1\leq i\leq n} |x_i|\]
	\item Two norms $||.||$ and $||.||'$ are said to be equivalent if there exist positive constants such that
	\[C' ||x|| \leq ||x||' \leq C ||x|| \qquad \text{for all }x\in R^n\]

	\item All norms are equivalent on $R^n$
	\[||x||_{l^\infty} \leq ||x||_{l^p} \leq n^\frac{1}{p} ||x||_{l^\infty} \]
	\[||x||_{l^2} \leq ||x||_{l^1} \leq \sqrt{n}||x||_{l^2} \]

	\end{itemize}

\subsection{Norms on matrix $M_{m\times n}$}
	\begin{itemize}
	\item Treating matrix as a $mn$ vector, 
	\[||A||_{l^p} = \left( \sum_{i=1}^m \sum_{j=1}^n |A_{ij}|^p \right)^\frac{1}{p} \]
	\[||A||_{l^\infty} = \max_{1\leq i\leq m, 1 \leq j \leq n} |A_{ij}| \]

	\item norm can be defined as ($l^{p}$ norm on each column and $l^q$ norm on the resultant vector)
	\[||A||_{L_{p,q}} = \left( \sum_{j=1}^n \left(\sum_{i=1}^m |A_{ij}|^p \right)^\frac{q}{p} \right)^\frac{1}{q} \]

	\item Taking $p=2$, we get \textbf{frobenius norm}
		\[||A||_{l^2} = \left( \sum_{j=1}^n \sum_{i=1}^m |A_{ij}|^2  \right)^\frac{1}{2}  \]

	\item \textbf{Matrix Norm}: A norm $||.||$ is said to be matrix norm if 
		\[||AB|| \leq ||A|| ||B|| \qquad \text{for all } A,B\in M_n(R)\]
	Frobenius norm is a matrix norm
	\[||AB||_{l^2} \leq ||A||_{l^2} ||B||_{l^2}\]

	\item \textbf{Subordinate Matrix Norm}: Let A be a $M_n$ matrix, then 
		\[\boxed{||A||_p = \sup_{x\in R^n-\{0\}} \frac{||Ax||_{l^p}}{||x||_{l^p}} =  \sup_{y\in R^n, ||y||=1} {||Ay||_{l^p}}} \]
	\item Subordinate Matrix norm is a matrix norm

	\item Subordinate matrix norm of identity matrix is always 1
		\[||I_n|| = \sup_{x\in R^n-\{0\}} \frac{||I_nx||}{||x||} =1\]

	\item Frobenius norm is a matrix norm but not a subordinate matrix norm (since frobenius norm of identity matrix is not 1)

	\item Subordinate matrix norm of unitary matrix ($A^T = A^{-1}$) is 1

	\item $||.||_2$ norm is invariant is invariant under multiplication by unitary matrices ($A^T = A^{-1} $)
		\[||AB||_2 = ||BA||_2 = ||B||_2 \]

	\item For a diagonal matrix $A = diag(a_1,a_2,\hdots,a_n)$
		\[||A||_2 = \max_{1 \leq i\leq n} |a_i|\]

	\end{itemize}

\subsection{Normal Matrices}
	A matrix $A\in M(C)$ is called a normal matrix
	\[AA^* = A^*A\]
	where $A^*$ is the adjoint of A 

	\begin{theorem}
		A matrix $A \in M(C)$ is a normal matrix iff it can be expressed as 
		\[A = U diag(\lambda_1,\lambda_2,\hdots,\lambda_n) U^*\]
		where $\lambda_1,\lambda_2,\hdots,\lambda_n$ are eigenvalues of A and U is a unitary matrix
	\end{theorem}

	\begin{itemize}
	\item For a normal matrix $A\in M_n(C)$
	\[||A||_2 = ||Udiag(\lambda_1,\lambda_2,\hdots,\lambda_n) U^*||_2 = \max_{1\leq i\leq n} |\lambda_i|\]
	\end{itemize}

\subsection{Spectral Radius}
	The maximum of the eigen values of a matrix $A$ is calle the spectral radius
		\[\rho(A) = \max_{1\leq i\leq n} |\lambda_i|\]

	\begin{itemize}
	\item For a normal matrix A, $||A||_2 = \rho(A)$
	\item For any \textbf{matrix norm} $||.||$ on $M(C)$, we have
		\[\rho(A)\leq ||A||\]

	\item Note : The above inequality may not hold for non matrix norm

	\begin{theorem}
	For any matrix $A\in M_n$ and for any $\epsilon > 0$, there exists a subordinate matrixnorm $||.||$ such that
		\[||A|| \leq \rho(A) + \epsilon\]
	This matrix norm depends on A and $\epsilon$
	\end{theorem}

	\end{itemize}


\subsection{Hermitian Matrix}
	\begin{itemize}
	\item A matrix is said to be hermitian if
		\[A = A^*\]
	\item A matrix is hermitian iff there exist unitary matrix U and real eigenvalues $\lambda_1,\lambda_2,\hdots,\lambda_n$, such that 
		\[A = Udiag(\lambda_1,\lambda_2,\hdots,\lambda_n) U^*\]

	\item For any matrix A, $AA^*$ is a hermitian matrix and also its eigenvalues are always positive. 
	\item \textbf{Singular Values}: The singular values of a matrix are the square roots of eigenvalues of $AA^*$.
	\item For a normal matrix, singular values are the modulli of its eigenvalues.

	\end{itemize}

\subsection{Subordinate matrix norm computation}
	\begin{itemize}
	\item Subordinate matrix norm $||.||_1$ (also called column sum)
		\[||A||_1 = \max_{1\leq j\leq n} (\sum_{i=0}^n A_{ij})\]
	i.e, $||.||$ is same as the maximum column sum (absolute values)

	\item Subordinate matrix norm $||.||_\infty$ 
		\[||A||_\infty = \max_{1\leq i\leq n} (\sum_{j=0}^n A_{ij})\]
	i.e, $||.||$ is same as the maximum row sum (absolute values)

	\item For any matrix $A \in M_n(C)$, subordinate $||A||_2$ norm is
		\[||A||_2 = \text{largest singular value of A} \]

	\end{itemize}

\section{Matrix Sequences}
	\subsection{Convergence}
	A sequence $A_k$ is said to converge to a limit A if
		\[\lim_{k\rightarrow \infty} ||A^{(k)}-A|| =0\]
	The choice of norm is irrelevant as all norms are equivalent

	\begin{theorem}
		The following are equivalent
		\begin{enumerate}
			\item $\lim_{k\rightarrow \infty} A^k = 0$
			\item $\lim_{k\rightarrow \infty} A^kx = 0$
			\item The spectral radius of $\rho(A) \leq 1$
			\item there exists at least one subordinate matrix such that $||A|| \leq 1$
		\end{enumerate}
	\end{theorem}

	\begin{theorem}
		\begin{enumerate}
		\item The geometric series $\sum_{k=0}^\infty A^k$ converges iff the $\rho(A)<1$
		\item If the geometric series converges, then 
			\[\sum_{k=0}^\infty A^k = (I-A)^{-1}\]
		\end{enumerate}
	\end{theorem}

	Miscellaneous note: If all the eigen values are non zero, then the matrix is invertible

	\begin{theorem}
		\item For any matrix B in the neighbourhood of A, i.e,
			\[||A-B|| < \frac{1}{||A^{-1}||}\]
		then B is invertible
	\end{theorem}

	\subsection{Relative Error}
	For a vector $y \in C^n$ and $\tilde{y}\in C^n$ its pertubed value, relative is error is defined as
		\[\text{Relative Error } = \frac{||y-\tilde{y}||}{||y||}\]

	\subsection{Condition Number}
	For a invertible matrix A, condition number is defined as
		\[cond(A) = ||A|| ||A^{-1}||\]

	\begin{itemize}
		\item $cond(A) \geq 1$
		\item $cond(A)= cond(A^{-1})$
		\item $cond(\alpha A) = cond(A)$ for all $\alpha \neq 0$\\

		% \item Suppose A is invertible and (A+B) is invertible then 
		% 	\[(A+B)^{-1} = (I_n+A^{-1}B)^{-1}A^{-1} \]

		\item Condition number acts as a amplifying factor - Let $A_\epsilon = A + \epsilon B$ for some B and $\epsilon << 1$, $b_\epsilon = b + \epsilon c$ for some $c\in C^n$
			\[\frac{||x_\epsilon - x||}{||x||} \leq ||A|| ||A^{-1}|| \left( \frac{||b_\epsilon - b||}{||b||} + \frac{||A_\epsilon - A||}{||A||}  \right) + O(\epsilon^2) \]

		\item A matrix is said to be well conditioned if $cond(A) \approx 1$ and ill conditioned if $cond(A) >>1$

		\item Consider a matrix A with $\mu_{max}$ and $\mu_{min}$ be its maximum and minimum singular value, then
			\[cond_2(A) = \frac{\mu_{max}}{\mu_{min}}\]

	\end{itemize}






























\end{document}