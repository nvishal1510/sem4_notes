\documentclass{article}
\title{MA 214 - Introduction to Numerical Analysis}
\author{Vishal Neeli}
\date{}

\usepackage[a4paper, total={6in, 11in}]{geometry}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{grffile}
\usepackage{physics}
\usepackage{amsfonts}
\usepackage{amsthm}
\hypersetup{
	colorlinks=true,
	urlcolor=blue,
	linkcolor=cyan,
	filecolor=red
}
\newtheorem*{theorem}{Theorem}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

% FOR CODE
% \usepackage{listings}
% \usepackage{color}

% \definecolor{dkgreen}{rgb}{0,0.6,0}
% \definecolor{gray}{rgb}{0.5,0.5,0.5}
% \definecolor{mauve}{rgb}{0.58,0,0.82}

% \lstset{frame=tb,
%   language=Java,
%   aboveskip=3mm,
%   belowskip=3mm,
%   showstringspaces=false,
%   columns=flexible,
%   basicstyle={\small\ttfamily},
%   numbers=none,
%   numberstyle=\tiny\color{gray},
%   keywordstyle=\color{blue},
%   commentstyle=\color{dkgreen},
%   stringstyle=\color{mauve},
%   breaklines=true,
%   breakatwhitespace=true,
%   tabsize=3
% }

\begin{document}
\maketitle

\section{Interpolation Theory}

\subsection{Introduction}

\begin{itemize}

	\item Given finite set of points, reconstructing the original curve is interpolation.
	\item There will be obviously infinitely many curve.

	\item Interpolation problem\\
	Given n+1 real distinct points: $x_0,x_1,\hdots,x_n$ and real numbers: $y_0,y_1,\hdots,y_n$

	Find a function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that 
	\[f(x_i)= y_i \qquad \text{for }i=0,1,\hdots,n\]
	Such a function is called \textbf{interpolant} and points $x_i$ are called \textbf{interpolation points}.\\
	We attempt to rebuild original function using polynomial functions. This is called polynomial interpolation and function is polynomial interpolant.

	\item A polynomial is function of the form 
	\[p(x)=a_0 + a_1 x +\hdots + a_n x^n\]
	\item $\mathbb{P}_n$ is the set of polynomials consisting of all polynomials of degree $\leq n$\\
\end{itemize}

\subsection{Polynomial Interpolation}
	\begin{theorem}[Joseph-Louis Lagrange Theorem]
		Given $n+1$ data points with unique $x_i$s, then there exists a unique polynomial $p_n \in \mathbb{P}_n$ such that 
		\[p(x_i)=y_i \qquad \text{for } i= 0,1,\hdots,n \]\\
	\end{theorem}
	\begin{proof}
		 (1)  This can be shown by linear algebra. In a $n$ degree polynomial, we substitute the points and get $n+1$ equations in n+1 variables (coeff) and all the rows are unique (since $x_0,x_1,\hdots,x_n$ are unique), hence in $AX=b$, $|A|\neq0$. \\
	\end{proof}

	\begin{proof}
		(2)  Part 1: Uniqueness : If there is an interpolant, then the interpolant is unique\\
		Let there be 2 interpolants, $p_n$ and $q_n$ and let $r(x)=p(x)-q(n)$,\\
		\[r(x)=0 \text{ for } i=0,1,\hdots,n\] 
		This contradicts the fundamental theorem of Algebra. (A polynomial of degree n can have at most n real roots). Therefore 
		\[r(x)=0 \quad \forall x \in \mathbb{R}\]
		\[p(x)=q(x) \quad \forall x \in \mathbb{R}\]

		\noindent Part 2: Existence (construction):\\
			Given n+1 data points, build n+1 Langrange polynomials 
			\[L^n_k (x) =
			\begin{cases}
			 0 \qquad \text{for } i\neq k \\
			 1 \qquad \text{for } i=k
			\end{cases}
			 \]

		\[L^n_k(x)= \frac{(x-x_0)...(x-x_{k-1})(x-x_{k+1})...(x-x_n)}{(x_k-x_0)...(x_k-x_{k-1})(x_k-x_{k+1})...(x_k-x_n)}\]

		\[p(x) = \sum_{k=0}^n y_k L^n_k(x)\]\\
	\end{proof}

\subsection{Closeness between functions}
	Given two continuous functions $f,g:[a,b]\rightarrow \mathbb{R}$, to evaluate how close the functions are consider the following
	\[max_{x\in [a,b]} |f(x)-g(x)|\]


\subsection{Set of continuous Functions}
	$C[a,b]$ is the set of all continuous functions on [a,b]\\
	$C[a,b]$ is a infinite dimensional vector space
	\[f,g \in C \Longrightarrow f+g \in C \text{  and  }\lambda f\in C\]
	We define norm on $C[a,b]$ as
	\[||f|| = max_{x\in [a,b]} |f(x)|\]
	$C^{k}[a,b]$ denotes the set of all functions which are continuously k-times differentiable  



\subsection{Polynomial Approximation and Error}
	\begin{theorem}[Weierstrass approximation Theorem]
	\label{weierstrass}
		Given a function $f\in C[a,b]$ and given $\epsilon>0$, there exists a polynomial $p(x)$ such that,
		\[||f(x)-p||<\epsilon\]
	\end{theorem}


	\noindent\textbf{Using Langrange's recipe to approximate}\\
	Take $n+1$ interpolation points in the [a,b] and collect the function values at all the points. We have $n+1$ data points. Using Lagrange polynomials, find the interpolant\\
		

	\begin{theorem}[Error equation]
		Let $f\in C^k[a,b]$, $x_0,x_1,\hdots,x_n \in [a,b]$ and $p\in \mathbb{P}_n$ be the interpolant using these points, then for all $x$, there exists a $\zeta = \zeta(x) \in (a,b)$ such that
		\[\boxed{f(x)-p(x) = \frac{1}{(n+1)!}f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)}\]
		\textbf{Note}: Here $\zeta$ is dependent on the x, i.e, for every x you choose, $\zeta$ generally changes.\\
	\end{theorem}

	\begin{proof}
		Consider the function,
		\[\psi(t)=(f(t)- p(t))\prod_{k=0}^n(x-x_k) - (f(x)-p(x))\prod_{k=0}^n(t-x_k)\]
		This $n+2$ roots (n+1 data points and x), applying rolle theorem's gives us that $\phi^{(1)}(t)$ has at least n+1 roots. Applying like this repeatedly on its derivatives, we get that $f^{(n+1)}$ has at least 1 root in $[a,b]$. Assuming the root to be $\zeta$. We have,
		\[f(x)-p(x) = \frac{1}{(n+1)!}f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)\]
	\end{proof}


	\noindent\textbf{Approximating the error}:\\
	Taking norm on both the sides of error equation, we have,
	\begin{align}
		\max_{x\in [a,b]} |f(x)-p(x)| &= \frac{1}{(n+1)!}||f^{(n+1})(\zeta)\prod_{k=0}^n(x-x_k)||\\
		\max_{x\in [a,b]} |f(x)-p(x)| &\leq \frac{1}{(n+1)!} ||f^{(n+1)}||\text{ }max_{x \in [a,b]}\prod_{k=0}^n(x-x_k)
	\end{align}

	\noindent\textbf{Chebyshef interpolation points}:
		\[x_k= \frac{a+b}{2}+\frac{b-a}{2}cos\left(\frac{j\pi}{n}\right)\]
	\noindent These points minimise $max_{x \in [a,b]}\prod_{k=0}^n(x-x_k)$ and therefore prefered over equally spaced points on real line. These points can be visualised as projections of equally spaced points on the arc of the semicircle with $\frac{a+b}{2}$ as center and $\frac{(b-a)}{2}$ as radius.



\subsection{Some more methods for calculating interpolant}
	\begin{itemize}
		\item This is similar to the linear algebra method (given as proof(1) to Joseph-Louis Lagrange Theorem) for finding the interpolant.\\
		Consider the polynomial 
		\[p(x)= a_0 +a_1 (x-x_0) + a_2(x-x_0)(x-x_1)+ \hdots +a_n(x-x_0)...(x-x_{n-1})\]
		Find the coefficients $a_0, a_1, \hdots, a_n$ by substituting the data points. On substituting $x_0$, we get $a_0$, again on substituting $x_1$ and using $a_0$, we get $a_1$ and so on.

		\item Interpolant $p(x)$ of  $x_0,x_1,\hdots,x_n$ can be calculated using interpolants $a(x)$ and $b(x)$ of $x_1,x_2,\hdots,x_n$ and $x_0,x_1,\hdots,x_{n-1}$ respectively as 
			\[p(x)=\frac{(x-x_0)a(x)-(x-x_n)b(x)}{x_n-x_0}\]
	\end{itemize}

\subsection{Divided difference - recursion relation}
	\begin{itemize}
		\item \textbf{Divided difference} : It is the coefficient of $x_n$ in the interpolant $p \in \mathbb{P}_n$ and denoted by $f[x_0,x_1\hdots,x_n]$.\\
		Using Langrange polynomials, we have
		\[p(x)=\sum_{k=0}^n f(x_k) \prod_{j=0, j\neq k}^n \frac{x-x_j}{x_k-x_j}\]
		So the divided difference is
		\[f[x_0,x_1,\hdots,x_n]=\sum_{k=0}^n f(x_k) \prod_{j=0, j\neq k}^n \frac{1}{x_k-x_j}\]

	\end{itemize}

	\begin{theorem}[Divided difference recursion theorem]
		\[\boxed{f[x_0,x_1,\hdots,x_{m+1}] = \frac{[f[x_1, x_2,\hdots,x_{m+1}] - f[x_0,x_1,\hdots,x_{m}]}{x_{m+1}-x_0}}\]
	\end{theorem}
	\begin{proof}
		Let p(x) be the interpolant for $x_0,x_1,\hdots,x_m$ and q(x) be the interpolant for $x_1,x_2\hdots,x_{m+1}$. Then,
		\[L(x)= \frac{(x-x_0)q(x)+(x_{m+1}-x)p(x)}{x_{m+1}-x_0}\] is an interpolant.
		Since, interpolant is unique, considering coeff of $x_{m}$ we have,
		\[f[x_0,x_1,\hdots, x_m]=  \frac{f[x_1, x_2,\hdots,x_{m+1}] - f[x_0,x_1,\hdots,x_{m}]}{x_{m+1}-x_0}\]
	\end{proof}

	\begin{theorem}[Interpolant using divided differences]
		Suppose $x_0,x_1,\hdots,x_n$ be the data points. Then interpolant $p \in \mathbb{P}_n$ is
		\[\boxed{p(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_n] \prod_{j=0}^{n-1}(x-x_j)}\]
	\end{theorem}

	\begin{proof}
		We prove this by induction. Base case $n=0$ is trivially satisfied.
		Assume that this is satisfied for $p_{k}$,
		\[p_k(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_k] \prod_{j=0}^{k-1}(x-x_j)\]
		Consider the polynomial $p_{k+1}(x)-p_k(x) \in \mathbb{P}_{k+1}$ which has $x_0,x_1,\hdots,x_k$ as roots. Hence,
		\[p_{k+1}(x)-p_k(x) = c\prod_{j=0}^k(x-x_j)\]
		Comparing leading coefficient on both sides, we have $c=f[x_0,x_1,\hdots,x_k]$. Hence,
		\[p_{k+1}(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_{k+1}] \prod_{j=0}^{k}(x-x_j)\]
		By PMI,
		\[p(x)=f[x_0]+f[x_0,x_1](x-x_0)+ \hdots +f[x_0,x_1,\hdots,x_n] \prod_{j=0}^{n-1}(x-x_j)\]


	\end{proof}


\subsection{Time complexity of the algorithms}
	% (Slide is useless)
	\begin{itemize}
		\item Langrange's method - $O(n^2)$ : Computing each Langrange polynomial can be done in $O(n)$ (Finding the coefficients given roots can be done in \href{https://cs.stackexchange.com/questions/116643/what-is-the-most-efficient-algorithm-to-compute-polynomial-coefficients-from-its}{$O(n log^2(n))$}). We need to do to this n times. So, $O(n^2)$.
		\item Divided differences - $O(n^2)$ : Summing operations in each stage - $n+(n-1)+ (n-2)+\hdots+ 1$. Hence, $O(n^2)$.
		\item Divided difference can be considered better because we can extend from n to n+1 and so on without discarding previous computation.
	\end{itemize}


\subsection{Weierstrass theorem consequences}
\begin{itemize}
	\item In \nameref{weierstrass}, take $\epsilon_n = \frac{1}{n}$. Then weierstrass theorem proves the existence of sequence of polynomials $p^{(1)}, p^{(2)}, \hdots$ such that 
		\[\lim_{n\rightarrow \infty} ||f-p^{(n)}||=0\]
	\item If \textbf{f in not a polynomial}, then 
		\[\lim_{n\rightarrow \infty} \text{degree of }p{(n)} = 0\]
		% To prove this (crudely), assume that there exist a non-polynomial function f such that $||f-p^{(n)}||=0$, then $f=p^{(n)}$ which contradicts the assumtion that f is not a polynomial.
\end{itemize}



\subsection{Spline Interpolation}
\label{spline}
\begin{itemize}
	\item \textbf{Piece wise polynomial}: $\phi \in C[a,b]$ is a piecewise polynomial function, if there exists $a=x_0<x_1<\hdots<x_n=b$ such that $\phi \in \mathbb{P}_m$ when $x \in [x_i,x_{i+1}]$ for all $i=0,1,\hdots,n$ and some $m>0$.


	\item Piece wise polynomial $\phi$ need not be polynomial in whole domain.

	\item Splines interpolation for $f \in C[a,b]$
	\begin{itemize}
		\item Pick some data points $x_0,x_1,\hdots,x_n$ such that $a=x_0<x_1<\hdots<x_n=b$
		\item Fix $m\leq n$
		\item Build $\phi$ in each subinterval $[x_i,x_{i+1}]$ using the following conditions:
			\[\phi(x_i)= f_i \qquad \text{for } i=0,1,\hdots,n\]
			\[\lim_{h\rightarrow 0+} \phi(x_i-h) = \lim_{h\rightarrow 0+} \phi(x_i+h) \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\lim_{h\rightarrow 0+} \dv{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\lim_{h\rightarrow 0+} \dv[2]{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv[2]{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
			\[\vdots\]
			\[\lim_{h\rightarrow 0+} \dv[m-1]{\phi(x_i-h)}{x} = \lim_{h\rightarrow 0+} \dv[m-1]{\phi(x_i+h)}{x} \qquad \text{for } i=1,2,\hdots,n-1\]
		\item We have $(n+1)+m(n-1)=n(m+1)-(m-1)$ conditions. We need $m-1$ more conditions.
	\end{itemize}

\end{itemize}


\subsection{Linear Splines}

	\textbf{Constructing linear splines}:\\
	Since the degree is only 1, we can constructs the splines using the equation of straight lines between the knots (data points).

	\begin{align*}
		s_0 &= \frac{f_1 - f_0}{x_1 - x_0}x + \frac{x_1 f_0 - x_0f_1}{x_1-x_0}		&\text{for } x\in [x_0,x_1]\\	
		s_1 &= \frac{f_2 - f_1}{x_2 - x_1}x + \frac{x_2 f_1 - x_1f_2}{x_2-x_1}		&\text{for } x\in [x_1,x_2]\\	
		&\vdots\\
		s_{n-1} &= \frac{f_n - f_{n-1}}{x_n - x_{n-1}}x + \frac{x_n f_{n-1} - x_{n-1}f_n}{x_n-x_{n-1}}		&\text{for } x\in [x_{n-1},x_n]\\	
	\end{align*}

	\begin{theorem}[Linear Splines error]
	\label{linear}
		Let $f\in C^2[a,b]$ and $s_L(x)$ be the interpolating \textbf{linear spline} at (n+1) knots $a=x_0<x_1<\hdots<x_n=b$ and let $h$ be the maximum subinterval length, then
			\[\boxed{||f-s_L|| \leq \frac{h^2}{8} ||f''||}\] 
	\end{theorem}
	\begin{proof}
		Consider the interval $[x_i,x_{i+1}]$, then $s_L (x)$ is the interpolating polynomial in this interval. Using the error equation for interpolating polynomials,
		\[f(x)-s_L(x)=\frac{1}{2} f''(\zeta) (x-x_i)(x-x_{i+1})\]
		Taking absolute value on both the sides,
		\begin{align*}
			|f(x)-s_L(x)| &= \frac{1}{2} |f''(\zeta)| |(x-x_i)(x-x_{i+1})|\\
						&\leq \frac{1}{2} ||f''|| \frac{h_i^2}{4} \qquad \text{where $h_i = \frac{x_{i+1}-x_i}{2}$ }\\
						&\leq \frac{h_i^2}{8} ||f''||
		\end{align*}
		Considering $h=max (h_i)$, then for $x\in [a,b]$
			\[max_{x\in [a,b]}|f(x)-s_L(x)| \leq \frac{1}{8} h^2||f''||\]

	\end{proof}



\subsection{Cubic Splines}
	\textbf{Constructing cubic splines}:\\
	\[s_i(x) = a_ix^3 + b_ix^2 + c_ix +d_i\]
	Using the conditions given in the section \ref{spline}, we have 4n-2 equations to for 4n coefficients. We choose the other two conditions as 
	\[s_0''(x_0) = s_{n-1}''(x_n)=0\]
	We have 4n variables (coefficients) and 4n equations, i.e, we have a $4n\times4n$ matrix which can be solved to get the coefficients of the spline.\\


	We can simplify this by choosing the form of spline as
	\[s_i(x) = a_i(x-x_i)^3 + b_i(x-x_i)^2 + c_i(x-x_i) +d_i \qquad \text{for }i=0,1,\hdots,n-1\]
	We will work only with \textbf{equally spaced knots}, i.e, $x_{i+1}-x_{i} = const$ for $i = 0, 1,\hdots,n-1$.
	We also define 
	\[\sigma_i = s''(x_i) \qquad \text{for }i=0,1,\hdots,n\]
	After doing a lot of simplification, we get 
	\begin{gather*}
		\boxed{a_i = \frac{\sigma_{i+1}-\sigma_i}{6h}}\\
		\boxed{b_i = \frac{\sigma_i}{2}}\\
		\boxed{c_i = \frac{f_{i+1}-f_i}{h} - \frac{h}{6}(2\sigma_i+\sigma_{i+1})}\\
		\boxed{d_i = f_i}
	\end{gather*}
	$\sigma_i$ s can be obtained by solving these equations
	\[\boxed{\sigma_{i-1} + 4 \sigma_i + \sigma_{i+1} = \frac{6}{h^2}(f_{i-1} - 2f_i + f_{i+1} )}\]
	This can be put in matrix form as
	\begin{equation*}
	\begin{bmatrix}
	4 &1 &0 &\hdots &0 &0 &0\\
	1 &4 &1 &\hdots &0 &0 &0\\
	0 &1 &4 &\hdots &0 &0 &0\\
	\vdots &\vdots &\vdots &\vdots &\vdots &\vdots &\vdots\\
	0 &0 &0 &\hdots &4 &1 &0\\
	0 &0 &0 &\hdots &1 &4 &1\\
	0 &0 &0 &\hdots &0 &1 &4
	\end{bmatrix} 
	\begin{bmatrix}
	\sigma_1\\
	\sigma_2\\
	\sigma_3\\
	\vdots\\
	\sigma_{n-3}\\
	\sigma_{n-2}\\
	\sigma_{n-1}
	\end{bmatrix}
	= \frac{6}{h^2}
	\begin{bmatrix}
	f_0 - 2f_1 +f_2\\
	f_1 -2f_2 +f_3\\
	f_2 -2f_3 +f_4\\
	\vdots\\
	f_{n-4} -2f_{n-3} +f_{n-2}\\
	f_{n-3} -2f_{n-2} +f_{n-1}\\
	f_{n-2} -2f_{n-1} +f_{n}
	\end{bmatrix}
	\end{equation*}


	\begin{theorem}[Error - Cubic Splines (equispaced knots)]
		Let $f\in C^4[a,b]$ and $s(x) \in C^2[a,b]$ be the interpolating \textbf{natural cubic spline} at (n+1) \textbf{equispaced knots} $a=x_0<x_1<\hdots<x_n=b$ and let $h$ be the subinterval length ($h=x_{i+1}-x_i$), then
			\[\boxed{||f-s|| \leq C||f^{(4)}|| h^4 \qquad \text{for some } C>0 }\] 
	\end{theorem}
	\begin{proof}
		Consider the function g which is defined as $g=f-s_i$ on each subinterval $[x_i,x_{i+1}]$.
		Then $g(x_i)=0$ for $i=0,1,\hdots,n-1$.\\
		We can see that the zero polynomial is the linear spline of $g(x)$ and using the theorem \nameref{linear}, we have
		\begin{align*}
			||g-0|| &\leq \frac{h^2}{8} ||g''||\\
            ||f-s|| &\leq \frac{h^2}{8} ||f'' - s''||
		\end{align*}
		Assuming that $||f''-s''||\leq C h^2 ||f^{(4)}||$, we have
		\[||f-s|| \leq C h^4 ||f^{(4)}|| \qquad \text{for some } C> 0\]
		
	\end{proof}

	\pagebreak





\section{Numerical Integration}
	\subsection{Introduction}
		\begin{itemize}
			\item Given f be a real valued function, we want to evaluate 
			\[\int_a^b f(x) dx\]

			\item If we can find its antiderivative $F(x)$, then we can use the fundamental theorem of calculus and evaluate it. But finding antiderivate is not so straightforward.
			\[\int_a^b f(x) dx = F(b) - F(a)\]
		\end{itemize}

	\subsection{Newton-Cotes Formula}
		Let $f(x):[a,b]\rightarrow \mathbb{R}$ and $p(x) \in \mathbb{P}_n$ be the polynomial interpolant using the data points $a=x_1<x_2<\hdots<x_n=b$, then the definite integral $\int_a^b f(x) dx$ can be approximated as 

		\begin{align*}
			\int_a^b f(x) dx &\approx \int_a^b p(x) dx\\
							 &= \int_a^b \sum_{i=0}^n f(x_i) L_i(x) dx\\
							 &= \sum_{i=0}^n f(x_i) \int_a^b L_i(x) dx
		\end{align*}
		Let $x_i = a+ih$ for $i=0,1,\hdots,n$ and $x= a + th$ for $t\in [0,n]$, then  we have,
		\[\int_a^b L_i(x) dx= \int_a^b \prod_{k=0, k\neq i}^n \frac{x-x_k}{x_i-x_k} dx = \int_a^b  \prod_{k=0, k\neq i}^n \frac{t-k}{i-k} h dt = h\int_a^b \varphi_i(t) dt = h w_i\]
		\[\text{where }w_i = \int_a^b \varphi_i(t) dt\text{    and   }\varphi_i(t) = \prod_{k=0, k\neq i}^n \frac{t-k}{i-k}\]
		\noindent Then,
		\[\boxed{\int_a^b f(x)dx = h \sum_{i=0}^n w_i f(x_i)} \]



		\noindent \textbf{Trapezium rule} ($n=1$):
			\[\int_a^b f(x)dx \approx \frac{b-a}{2} (f(a)+f(b))\]
		\textbf{Simpson's rule} ($n=2$):
			\[\int_a^b f(x)dx \approx \frac{h}{3} \left(f(a) + 4f\left(\frac{a+b}{2}\right)+ f(b)\right)\]





































































\end{document}